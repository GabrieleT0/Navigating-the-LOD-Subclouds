# Navigating the LOD Subclouds: Assessing Linked Open Data Quality by Domain
This repository contains code and supporting data for the publication "*Navigating the LOD Subclouds: Assessing Linked Open Data Quality by Domain*", submitted as a full research paper to "*The Web of Data Quality Workshop@The Web Conference 2025*"
The paper idea is to evaluate the quality of the different domains (also called sub-clouds) within the [Linked Open Data Cloud](https://lod-cloud.net/), utilizing quality data computed by [KGHeartBeat](https://doi.org/10.1007/978-3-031-77847-6_3).
The quality data calculated by the tool, as well as the quality evaluation data for individual sub-clouds and the corresponding boxplots, are included within this project, refer to the [How to Explore Evaluation Data](#how-to-explore-evaluation-data) section for an overview of the repository structure and guidance on locating the evaluation data and charts within this project. This repository also includes the quality data extracted from [Schmachtenberg_et_al.](https://doi.org/10.1007/978-3-319-11964-9_16) and [State of the LOD Cloud](https://web.archive.org/web/20160323120153/lod-cloud.net/state/#structure), used as references for the publictions.


# How to Explore Evaluation Data üîç

In the [data](./data) folder, the file [lodcloud.json](./data/lodcloud.json) represents the snapshot of the LOD Cloud as of November 24, 2024. The file [kgs_by_topic.json](./data/kgs_by_topic.json) contains the Knowledge Graphs (KGs) categorized by domain, where each key corresponds to a sub-cloud, and the value is the list of KG IDs within that sub-cloud. This JSON file is generated by the script [split_lodc_kgs_by_topic.py](./src/split_lodc_kgs_by_topic.py).

In the [data/quality_data](./data/quality_data/) folder, you will find the quality data computed with KGHeartBeat as of November 24, 2024. In the subfolder [data/quality_data/all_kgs_analyzed](./data/quality_data/all_kgs_analyzed/), there is the complete output file generated by the tool, containing the analysis of all KGs processed by the tool. Meanwhile, in the subfolder [data/quality_data/only_from_LODC](./data/quality_data/only_from_LODC/), you will find the quality data of KGs categorized by their domain, each within a separate folder. Due to size constraints, the files in these subfolders have not been included in the repository but can be regenerated by rerunning the code as described in the [How to Reproduce the Evaluation](#how-to-reproduce-the-evaluation) section.

The [data/evaluation_results](./data/evaluation_results/) contains all the evaluation data used to generate the tables and charts presented in the article.

In the subfolder [data/evaluation_data/evaluation_results/Schmachtenberg_et_al.-2014](./data/evaluation_results/Schmachtenberg_et_al.-2014/), you will find the evaluation data extracted from the work of[ Schmachtenberg et al.](https://doi.org/10.1007/978-3-319-11964-9_16) Similarly, the subfolder data/[evaluation_results/State-of-the_LODCloud-2011](./data/evaluation_results/State-of-the-LODCloud-2011/) contains evaluation data retrieved from the [Web Archive page](https://web.archive.org/web/20160323120153/lod-cloud.net/state/#structure). The file [2011-vs-2014-vs-2024.csv](./data/evaluation_results/2011-vs-2014-vs-2024.csv) provides a comparative table of the evaluation data for the LOD Cloud from 2011, 2014, and 2024.

Finally, in the folder [data/evaluation_results/2024](./data/evaluation_results/2024/), you will find the evaluation data derived from the quality data computed by KGHeartBeat. The data is organized into subfolders based on the corresponding domain. Within each domain folder, there is a [punctual](./data/evaluation_results/2024/cross-domain/) subfolder containing CSV files with the data. 

Key files in the data/evaluation_results/2024/cross-domain/punctual folder include (we use cross-domain as an example but all the different domain folders follow the same structure and use identical file names for the CSV files):
- [Availability of RDF dump (metadata)_evaluation.csv](./data/evaluation_results/2024/cross-domain/punctual/Availability%20of%20RDF%20dump%20(metadata)_evaluation.csv): Contains data regarding the number of available RDF dumps.
- [Availability VoID file_evaluation.csv](./data/evaluation_results/2024/cross-domain/punctual/Availability%20VoID%20file_evaluation.csv): Provides values related to the availability of VoID files.
- [License machine readable (metadata)_evaluation.csv](./data/evaluation_results/2024/cross-domain/punctual/License%20machine%20redeable%20(metadata)_evaluation.csv): Contains information about the number of KGs specifying a machine-readable license.
- [Sparql endpoint_evaluation.csv](./data/evaluation_results/2024/cross-domain/punctual/Sparql%20endpoint_evaluation.csv): Includes details about the status of SPARQL endpoints for KGs within the sub-cloud.
- [categories_stats.csv](./data/evaluation_results/2024/cross-domain/punctual/categories_stats.csv): Reports the minimum, Q1, median, Q3, maximum, and mean values for six different quality categories. These data are essential for generating boxplots.
- [dimensions_stats.csv](./data/evaluation_results/2024/cross-domain/punctual/dimensions_stats.csv): Provides the minimum, Q1, median, Q3, maximum, and mean values for 20 different quality dimensions. These data are also used for creating boxplots.

# How to explore evaluation data plots üìä

In the [charts](./charts/) folder, you can review the boxplots generated from the evaluation data of the 2024 LOD Cloud. This folder contains several subfolders, each named after a specific sub-cloud, and each subfolder includes evaluation charts specific to that sub-cloud.

For example, in the [cross-domain/punctual](./charts/cross-domain/punctual/) folder, there are two key files:
- [quality_categories.png](./charts/cross-domain/punctual/quality_categories.png): A boxplot showing the evaluation data for the six quality categories.
- [quality_dimensions.png](./charts/cross-domain/punctual/quality_dimensions.png): A boxplot displaying the quality data for the 20 quality dimensions within that domain.

All the domain subfolders follow the same structure with the same filename for the plots.

The [by_domain](./charts/by_domain/) folder contains comparative charts that fix a specific [quality category](./charts/by_domain/by_categories/) (by_categories subfolder) or [dimension](./charts/by_domain/by_dimensions/) (by_dimensions subfolder) and illustrates how the quality varies across different domains. For instance:
	‚Ä¢	[Accessibility.png](./charts/by_domain/by_categories/Accessibility.png): This boxplot presents the quality of the Accessibility category across the nine sub-clouds, including "All". The "All" values represent evaluation data obtained for all KGs in the LOD Cloud without domain distinction, reflecting the general quality of that category across the entire cloud.

# How to Reproduce the Evaluation üöÄ
To reproduce the experiment and regenerate the evaluation data and charts, it is necessary to extract the ZIP file containing the quality data for all KGs analyzed by KGHeartBeat. This file is located in the [data/quality_data/all_kgs_analyzed](./data/quality_data/all_kgs_analyzed) folder.

Once the file is extracted, proceed with creating a virtual environment (recommended) and then install the required dependencies by using the provided requirements file.

#### Creates a virtual environment (recommended but not required) and installs all the dependencies

```sh
#Create the virtual environment
python<version> -m venv <virtual-environment-name>

#Activation of the environment
source <virtual-environment-name>/bin/activate 
# or for Windows users
<virtual-environment-name>/Scripts/activate.bat //In CMD
<virtual-environment-name>/Scripts/Activate.ps1 //In Powershel

#Install all the dependencies
pip install -r requirements.txt
```

#### Install all the dependencies
```sh
pip install -r requirements.txt
```

#### Execute the main.py script
At this point, you can run the ```main.py``` file to perform the quality evaluation of the sub-clouds. Go to the [src](./src/) folder and execute the ```main.py``` file.

```sh
cd src
python<version> main.py
```

#### main.py options
There are several parameters that can be passed to the main.py script to modify its execution behavior.
```sh
python3 main.py --jump_filtering # If specified, the quality data from the directory ./quality_data will not be filtered by extracting only KGs from LOD Cloud

python3 main.py --charts_only # If specified, the script will only generate charts and skip other processing steps. 

python3 main.py --topics_only # If specified, the evaluation will be done by dividing KGs by topic, no overall analysis of the LOD Cloud will be done.

python3 main.py --all_lodc # If specified, the evaluation will be made of the quality of the entire LOD Cloud, without the breakdown of KGs by topic."
```

# Execute the Evaluation on New Quality Data üÜï
To evaluate the LOD Cloud and sub-clouds on new quality data computed by KGHeartBeat, follow these steps:
1. Download the CSV file with all KGs quality data computed by KGHeartBeat from the following link: [http://www.isislab.it:12280/kgheartbeat/](http://isislab.it:12280/kghb_analysis_data/) .
2. Replace the existing [2024-11-24.csv](./data/quality_data/all_kgs_analyzed/2024-11-24.csv.gz) file in the [data/quality_data/all_kgs_analyzed](data/quality_data/all_kgs_analyzed) folder with the new file. Ensure that you extract the downloaded KGHeartBeat file and only insert the CSV file, without modifying its name in any way.
3. If you want to use a new snapshot of the LOD Cloud, simply download the JSON file with the updated data from the LOD Cloud website. Replace the [lodcloud.json](data/lodcloud.json) file located in the [data](./data) folder with the new file, making sure to keep the same file name.

After this, run the main.py file to perform the evaluation from the [src](./src/) folder.

```sh
cd src
python<version> main.py
```
